{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Deep Learning\n",
    "## Modell Zusammenstellung und Training\n",
    "### SetUp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Jewellery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU use (before importing tensorflow backend):\n",
    "\n",
    "#from __future__ import print_function\n",
    "#import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 - Building the CNN\n",
    "import pandas\n",
    "# set Workingdirectory\n",
    "import os\n",
    "#os.chdir(r'Path\\to\\your\\pictures')\n",
    "\n",
    "import random\n",
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "#from google_images_download import google_images_download   #importing the library\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# CNN\n",
    "### Pretrained Convolutional Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base_vgg16 = VGG16(\n",
    "  weights = \"imagenet\",\n",
    "  include_top = False,\n",
    "  input_shape = (150, 150, 3)\n",
    ")\n",
    "\n",
    "conv_base_vgg16.trainable = False\n",
    "\n",
    "conv_base_vgg16.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 16,828,739\n",
      "Trainable params: 2,114,051\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(conv_base_vgg16)\n",
    "\n",
    "classifier.add(Flatten(input_shape=conv_base_vgg16.output_shape[1:]))\n",
    "\n",
    "# =============================================================================\n",
    "# selbst entworfene Convolutional Layers\n",
    "#\n",
    "# # Step 1 - Convolution\n",
    "# classifier.add(Convolution2D(128, kernel_size = (3,3), strides = (3, 3), input_shape = (150,150, 3), activation = 'relu'))\n",
    "# \n",
    "# # Step 2 - Pooling\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# \n",
    "# # Adding a second convolutional layer\n",
    "# classifier.add(Convolution2D(64, kernel_size = (3,3), strides = (3, 3), activation = 'relu'))\n",
    "# \n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# \n",
    "# # Step 3 - Flattening\n",
    "# classifier.add(Flatten())\n",
    "# \n",
    "# =============================================================================\n",
    "# Step 4 - Full connection\n",
    "\n",
    "classifier.add(Dense(units = 256, activation = 'relu'))\n",
    "\n",
    "classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "\n",
    "classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "#classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.compile(optimizer = RMSprop(lr = 2e-5), loss ='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "classifier.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 images belonging to 3 classes.\n",
      "Found 50 images belonging to 3 classes.\n",
      "{'Bracelet': 0, 'Necklace': 1, 'Ring': 2}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255,\n",
    "                                     rotation_range = 40,\n",
    "                                     width_shift_range = 0.2,\n",
    "                                     height_shift_range = 0.2,\n",
    "                                     shear_range = 0.2,\n",
    "                                     zoom_range = 0.2,\n",
    "                                     horizontal_flip = True,\n",
    "                                     vertical_flip = True,\n",
    "                                     fill_mode = \"nearest\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(name +'/training_set',\n",
    "                                                 target_size = [150,150],\n",
    "                                                 batch_size = 10, \n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(name + '/test_set',\n",
    "                                            target_size = [150,150],\n",
    "                                            batch_size = 10, \n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "classes = training_set.class_indices\n",
    "f = open(name + '/classes.txt','w')\n",
    "f.write(str(classes))\n",
    "f.close()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "i = 0\n",
    "try:\n",
    "    os.mkdir(name + '/batch_pics')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for batch in train_datagen.flow_from_directory(name + '/test_set',\n",
    "                                      target_size = (150,150),\n",
    "                                      batch_size = 10,\n",
    "                                      class_mode = 'categorical',\n",
    "                                      save_to_dir = name + '/batch_pics'):\n",
    "    i += 1\n",
    "    if i > 20: break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### checkpoint callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checkpoints des CNN(nach jeder Epoch wird gespeichert):\n",
    "checkpoint_dir = name + '/checkpoints'\n",
    "try:\n",
    "    os.makedirs(checkpoint_dir)\n",
    "except:\n",
    "    pass\n",
    "filepath = checkpoint_dir + \"/model.{epoch:02d}-{loss:.3f}-{acc:.2f}-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath = filepath,\n",
    "    save_weights_only = False,\n",
    "    verbose = 1,\n",
    "    save_best_only = True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-8fbc9baa0fda>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 15 steps, validate for 5 steps\n",
      "Epoch 1/40\n",
      "15/15 [==============================] - 9s 618ms/step - loss: 1.1846 - accuracy: 0.3862 - val_loss: 1.0262 - val_accuracy: 0.5600\n",
      "Epoch 2/40\n",
      "15/15 [==============================] - 8s 543ms/step - loss: 1.0171 - accuracy: 0.5379 - val_loss: 0.8958 - val_accuracy: 0.6200\n",
      "Epoch 3/40\n",
      "15/15 [==============================] - 9s 568ms/step - loss: 0.9264 - accuracy: 0.6000 - val_loss: 0.8177 - val_accuracy: 0.7200\n",
      "Epoch 4/40\n",
      "15/15 [==============================] - 8s 547ms/step - loss: 0.8322 - accuracy: 0.6759 - val_loss: 0.7635 - val_accuracy: 0.7600\n",
      "Epoch 5/40\n",
      "15/15 [==============================] - 9s 571ms/step - loss: 0.7598 - accuracy: 0.7379 - val_loss: 0.7386 - val_accuracy: 0.7000\n",
      "Epoch 6/40\n",
      "15/15 [==============================] - 8s 556ms/step - loss: 0.7231 - accuracy: 0.7379 - val_loss: 0.6577 - val_accuracy: 0.7600\n",
      "Epoch 7/40\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 0.6663 - accuracy: 0.8138 - val_loss: 0.6065 - val_accuracy: 0.8800\n",
      "Epoch 8/40\n",
      "15/15 [==============================] - 9s 570ms/step - loss: 0.6779 - accuracy: 0.7379 - val_loss: 0.5795 - val_accuracy: 0.8800\n",
      "Epoch 9/40\n",
      "15/15 [==============================] - 9s 579ms/step - loss: 0.6321 - accuracy: 0.8000 - val_loss: 0.5304 - val_accuracy: 0.8600\n",
      "Epoch 10/40\n",
      "15/15 [==============================] - 9s 580ms/step - loss: 0.6014 - accuracy: 0.7724 - val_loss: 0.5134 - val_accuracy: 0.8600\n",
      "Epoch 11/40\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 0.5607 - accuracy: 0.8000 - val_loss: 0.4758 - val_accuracy: 0.9000\n",
      "Epoch 12/40\n",
      "15/15 [==============================] - 9s 581ms/step - loss: 0.5513 - accuracy: 0.8621 - val_loss: 0.4893 - val_accuracy: 0.7600\n",
      "Epoch 13/40\n",
      "15/15 [==============================] - 9s 582ms/step - loss: 0.5069 - accuracy: 0.8414 - val_loss: 0.4231 - val_accuracy: 0.9000\n",
      "Epoch 14/40\n",
      "15/15 [==============================] - 8s 566ms/step - loss: 0.4698 - accuracy: 0.8759 - val_loss: 0.4124 - val_accuracy: 0.9000\n",
      "Epoch 15/40\n",
      "15/15 [==============================] - 8s 553ms/step - loss: 0.4923 - accuracy: 0.8483 - val_loss: 0.3882 - val_accuracy: 0.9000\n",
      "Epoch 16/40\n",
      "15/15 [==============================] - 9s 583ms/step - loss: 0.4684 - accuracy: 0.8345 - val_loss: 0.3983 - val_accuracy: 0.9000\n",
      "Epoch 17/40\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 0.4447 - accuracy: 0.8414 - val_loss: 0.3898 - val_accuracy: 0.9000\n",
      "Epoch 18/40\n",
      "15/15 [==============================] - 9s 573ms/step - loss: 0.3929 - accuracy: 0.8828 - val_loss: 0.4145 - val_accuracy: 0.9000\n",
      "Epoch 19/40\n",
      "15/15 [==============================] - 8s 559ms/step - loss: 0.3887 - accuracy: 0.9310 - val_loss: 0.3912 - val_accuracy: 0.9000\n",
      "Epoch 20/40\n",
      "15/15 [==============================] - 9s 588ms/step - loss: 0.4089 - accuracy: 0.8966 - val_loss: 0.3691 - val_accuracy: 0.9000\n",
      "Epoch 21/40\n",
      "15/15 [==============================] - 9s 568ms/step - loss: 0.4265 - accuracy: 0.8690 - val_loss: 0.3469 - val_accuracy: 0.9000\n",
      "Epoch 22/40\n",
      "15/15 [==============================] - 9s 571ms/step - loss: 0.3833 - accuracy: 0.8828 - val_loss: 0.3402 - val_accuracy: 0.9000\n",
      "Epoch 23/40\n",
      "15/15 [==============================] - 9s 569ms/step - loss: 0.3940 - accuracy: 0.8828 - val_loss: 0.4016 - val_accuracy: 0.9000\n",
      "Epoch 24/40\n",
      "15/15 [==============================] - 9s 570ms/step - loss: 0.3585 - accuracy: 0.8897 - val_loss: 0.4226 - val_accuracy: 0.9000\n",
      "Epoch 25/40\n",
      "15/15 [==============================] - 9s 568ms/step - loss: 0.3445 - accuracy: 0.8966 - val_loss: 0.4206 - val_accuracy: 0.8600\n",
      "Epoch 26/40\n",
      "15/15 [==============================] - 8s 553ms/step - loss: 0.3652 - accuracy: 0.8828 - val_loss: 0.3650 - val_accuracy: 0.8600\n",
      "Epoch 27/40\n",
      "15/15 [==============================] - 9s 576ms/step - loss: 0.2998 - accuracy: 0.9586 - val_loss: 0.3352 - val_accuracy: 0.9000\n",
      "Epoch 28/40\n",
      "15/15 [==============================] - 8s 552ms/step - loss: 0.3145 - accuracy: 0.9103 - val_loss: 0.3977 - val_accuracy: 0.8600\n",
      "Epoch 29/40\n",
      "15/15 [==============================] - 9s 568ms/step - loss: 0.3191 - accuracy: 0.8828 - val_loss: 0.4236 - val_accuracy: 0.8400\n",
      "Epoch 30/40\n",
      "15/15 [==============================] - 8s 550ms/step - loss: 0.3051 - accuracy: 0.9310 - val_loss: 0.3998 - val_accuracy: 0.8600\n",
      "Epoch 31/40\n",
      "15/15 [==============================] - 9s 574ms/step - loss: 0.3257 - accuracy: 0.8897 - val_loss: 0.4139 - val_accuracy: 0.8600\n",
      "Epoch 32/40\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 0.2908 - accuracy: 0.9103 - val_loss: 0.3974 - val_accuracy: 0.8600\n",
      "Epoch 33/40\n",
      "15/15 [==============================] - 9s 569ms/step - loss: 0.2907 - accuracy: 0.8966 - val_loss: 0.4345 - val_accuracy: 0.8600\n",
      "Epoch 34/40\n",
      "15/15 [==============================] - 8s 558ms/step - loss: 0.2522 - accuracy: 0.9310 - val_loss: 0.4109 - val_accuracy: 0.9000\n",
      "Epoch 35/40\n",
      "15/15 [==============================] - 9s 574ms/step - loss: 0.2865 - accuracy: 0.9379 - val_loss: 0.3541 - val_accuracy: 0.9000\n",
      "Epoch 36/40\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 0.3035 - accuracy: 0.8897 - val_loss: 0.3750 - val_accuracy: 0.9000\n",
      "Epoch 37/40\n",
      "15/15 [==============================] - 9s 567ms/step - loss: 0.2408 - accuracy: 0.9517 - val_loss: 0.4002 - val_accuracy: 0.9000\n",
      "Epoch 38/40\n",
      "15/15 [==============================] - 9s 572ms/step - loss: 0.2101 - accuracy: 0.9448 - val_loss: 0.3673 - val_accuracy: 0.9000\n",
      "Epoch 39/40\n",
      "15/15 [==============================] - 8s 564ms/step - loss: 0.2582 - accuracy: 0.9172 - val_loss: 0.3889 - val_accuracy: 0.9000\n",
      "Epoch 40/40\n",
      "15/15 [==============================] - 9s 577ms/step - loss: 0.2538 - accuracy: 0.9310 - val_loss: 0.3797 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a8b8cba888>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################\n",
    "random.seed(1)\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         #steps_per_epoch = 10,\n",
    "                         epochs = 40,\n",
    "                         validation_data = test_set,\n",
    "                         #callbacks = [cp_callback]\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifier.save(name + \"/checkpoints/cnn_model.hdf5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
